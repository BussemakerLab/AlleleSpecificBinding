{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asb\n",
    "import math\n",
    "import torch\n",
    "import pyprobound\n",
    "import pyprobound.plotting\n",
    "import pyprobound.utils\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = pyprobound.alphabets.DNA()\n",
    "dataframe = asb.get_asb_dataframe(\"CTCF.tsv\")\n",
    "asb_table = asb.ASBTable(\n",
    "    dataframe, alphabet, left_flank_length=29, right_flank_length=29\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': 'Tue Sep 26 11:40:26 2023',\n",
       " 'version': '1.0.0',\n",
       " 'flank_lengths': [(0, 0)]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PSAMs\n",
    "nonspecific = pyprobound.layers.NonSpecific(alphabet=alphabet, name=\"NS\")\n",
    "psam = pyprobound.layers.PSAM(\n",
    "    kernel_size=18,\n",
    "    alphabet=alphabet,\n",
    "    name=\"CTCF\",\n",
    ")\n",
    "psam.reload(\"../likelihood/data/bindingModels/motifcentral_fit_12715.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 9\n",
      "\t\tRight Flank Length: 9\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -2.944438934326172\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0.],\n",
      "\t\t\t                 [0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.09s NLL: 3.2355573177 Reg.: 0.0001756882 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.03s NLL: 3.2355573177 Reg.: 0.0001756882 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -2.9443585872650146\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0.],\n",
      "\t\t\t                 [0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -2.995651960372925\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 15.53247356414795\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0.],\n",
      "\t\t\t                 [0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.41s NLL: 3.2309780121 Reg.: 0.0002909339 Distance: 6.3655395508 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.13s NLL: 3.2309780121 Reg.: 0.0002909339 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3222455978393555\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True 1.5019859075546265\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 11.027892112731934\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0.],\n",
      "\t\t\t                 [0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 10\n",
      "\t\tRight Flank Length: 10\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.044522523880005\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.09s NLL: 3.2355573177 Reg.: 0.0001762875 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001762875 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.0444395542144775\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.0957329273223877\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 14.839326858520508\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucas/Optimer/Gits/PyProBound/src/pyprobound/optimizer.py:142: UserWarning: Checkpoint file test_lengths.pt is not empty\n",
      "  warnings.warn(f\"Checkpoint file {checkpoint} is not empty\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.89s NLL: 3.2259762287 Reg.: 0.0002824283 Distance: 5.8733305931 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.32s NLL: 3.2259762287 Reg.: 0.0002824283 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.338439702987671\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True 1.0562124252319336\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 10.685208320617676\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 11\n",
      "\t\tRight Flank Length: 11\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.1354942321777344\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.08s NLL: 3.2355573177 Reg.: 0.0001768497 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001768497 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.135408639907837\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.186702013015747\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 14.433865547180176\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.46s NLL: 3.2231647968 Reg.: 0.0002780006 Distance: 5.5562243462 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.06s NLL: 3.2231647968 Reg.: 0.0002780006 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3416402339935303\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True 0.7393893003463745\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 10.5023775100708\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 12\n",
      "\t\tRight Flank Length: 12\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.2188758850097656\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.09s NLL: 3.2355573177 Reg.: 0.0001773796 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001773796 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.2187881469726562\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.2700815200805664\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 14.146185874938965\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.54s NLL: 3.2178969383 Reg.: 0.0002761194 Distance: 5.2590484619 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.07s NLL: 3.2178969383 Reg.: 0.0002761194 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3535971641540527\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True 0.44729408621788025\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 10.426369667053223\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 13\n",
      "\t\tRight Flank Length: 13\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.295836925506592\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.09s NLL: 3.2355573177 Reg.: 0.0001778809 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001778809 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.2957470417022705\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.3470404148101807\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 13.923043251037598\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.79s NLL: 3.2118508816 Reg.: 0.0002753908 Distance: 4.9861588478 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.29s NLL: 3.2118508816 Reg.: 0.0002753908 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3677127361297607\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True 0.17725658416748047\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 10.39627742767334\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 14\n",
      "\t\tRight Flank Length: 14\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.367295742034912\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.09s NLL: 3.2355573177 Reg.: 0.0001783570 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001783570 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.367204189300537\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.4184975624084473\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 13.740723609924316\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.63s NLL: 3.2059345245 Reg.: 0.0002732603 Distance: 4.8757638931 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.09s NLL: 3.2059345245 Reg.: 0.0002732603 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.379845380783081\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True 0.0274908896535635\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 10.291996955871582\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 15\n",
      "\t\tRight Flank Length: 15\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.4339871406555176\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.09s NLL: 3.2355573177 Reg.: 0.0001788106 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001788106 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.4338936805725098\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.48518705368042\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 13.586575508117676\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.66s NLL: 3.2040743828 Reg.: 0.0002713263 Distance: 4.7937421799 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.10s NLL: 3.2040743828 Reg.: 0.0002713263 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3845298290252686\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -0.0970628559589386\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 10.196087837219238\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 16\n",
      "\t\tRight Flank Length: 16\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.4965076446533203\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.09s NLL: 3.2355573177 Reg.: 0.0001792438 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001792438 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.496412515640259\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.547705888748169\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 13.453045845031738\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.98s NLL: 3.2037074566 Reg.: 0.0002701975 Distance: 4.6875538826 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.43s NLL: 3.2037074566 Reg.: 0.0002701975 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3856918811798096\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -0.23429052531719208\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 10.13806438446045\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 17\n",
      "\t\tRight Flank Length: 17\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.5553481578826904\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.10s NLL: 3.2355573177 Reg.: 0.0001796587 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001796587 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.555251359939575\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.6065447330474854\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 13.335264205932617\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.89s NLL: 3.2033731937 Reg.: 0.0002692300 Distance: 4.5940804482 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.40s NLL: 3.2033731937 Reg.: 0.0002692300 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.386040687561035\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -0.3591458797454834\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 10.086482048034668\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 18\n",
      "\t\tRight Flank Length: 18\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.610917806625366\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.10s NLL: 3.2355573177 Reg.: 0.0001800569 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001800569 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.6108193397521973\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.6621127128601074\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 13.229904174804688\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 1.13s NLL: 3.2032439709 Reg.: 0.0002684179 Distance: 4.5091533661 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.59s NLL: 3.2032439709 Reg.: 0.0002684179 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3869998455047607\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -0.47486579418182373\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 10.041112899780273\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 19\n",
      "\t\tRight Flank Length: 19\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.6635615825653076\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.10s NLL: 3.2355573177 Reg.: 0.0001804399 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.03s NLL: 3.2355573177 Reg.: 0.0001804399 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.663461685180664\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.714755058288574\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 13.13459587097168\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 1.15s NLL: 3.2033236027 Reg.: 0.0002675583 Distance: 4.4421434402 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.46s NLL: 3.2033236027 Reg.: 0.0002675583 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3867642879486084\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -0.5750792622566223\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 9.99300765991211\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 20\n",
      "\t\tRight Flank Length: 20\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.7135720252990723\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.10s NLL: 3.2355573177 Reg.: 0.0001808088 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001808088 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.713470697402954\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.7647640705108643\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 13.047585487365723\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 1.48s NLL: 3.2032399178 Reg.: 0.0002669069 Distance: 4.3734102249 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.62s NLL: 3.2032399178 Reg.: 0.0002669069 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3868987560272217\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -0.6741958856582642\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 9.954110145568848\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 21\n",
      "\t\tRight Flank Length: 21\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.761200189590454\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.10s NLL: 3.2355573177 Reg.: 0.0001811648 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001811648 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.7610974311828613\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.8123908042907715\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 12.967545509338379\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 1.30s NLL: 3.2031619549 Reg.: 0.0002663563 Distance: 4.3092517853 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.44s NLL: 3.2031619549 Reg.: 0.0002663563 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.387075424194336\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -0.7670091986656189\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 9.919634819030762\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 22\n",
      "\t\tRight Flank Length: 22\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.8066625595092773\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.10s NLL: 3.2355573177 Reg.: 0.0001815088 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001815088 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.806558847427368\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.8578522205352783\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 12.893438339233398\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 1.96s NLL: 3.2031033039 Reg.: 0.0002660326 Distance: 4.2391219139 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.67s NLL: 3.2031033039 Reg.: 0.0002660326 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3869106769561768\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -0.8616250157356262\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 9.895563125610352\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 23\n",
      "\t\tRight Flank Length: 23\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.8501474857330322\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.10s NLL: 3.2355573177 Reg.: 0.0001818418 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001818418 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.8500425815582275\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.9013359546661377\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 12.824446678161621\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 1.17s NLL: 3.2026417255 Reg.: 0.0002655463 Distance: 4.1868619919 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.50s NLL: 3.2026417255 Reg.: 0.0002655463 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.387281656265259\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -0.9420943260192871\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 9.863512992858887\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 24\n",
      "\t\tRight Flank Length: 24\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.891820192337036\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.10s NLL: 3.2355573177 Reg.: 0.0001821644 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.03s NLL: 3.2355573177 Reg.: 0.0001821644 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.891713857650757\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.943007230758667\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 12.759908676147461\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 1.08s NLL: 3.2025473118 Reg.: 0.0002651737 Distance: 4.1337552071 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.66s NLL: 3.2025473118 Reg.: 0.0002651737 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3886337280273438\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -1.0215364694595337\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 9.836355209350586\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 25\n",
      "\t\tRight Flank Length: 25\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.931825637817383\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.10s NLL: 3.2355573177 Reg.: 0.0001824773 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.03s NLL: 3.2355573177 Reg.: 0.0001824773 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.931718349456787\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.9830117225646973\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 12.699285507202148\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 1.93s NLL: 3.2024815083 Reg.: 0.0002648686 Distance: 4.0817632675 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.70s NLL: 3.2024815083 Reg.: 0.0002648686 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3887994289398193\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -1.0983147621154785\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 9.81250286102295\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 26\n",
      "\t\tRight Flank Length: 26\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.97029185295105\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.11s NLL: 3.2355573177 Reg.: 0.0001827813 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001827813 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -3.9701836109161377\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -4.021476745605469\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 12.642127990722656\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 2.37s NLL: 3.2024943829 Reg.: 0.0002645317 Distance: 4.0368094444 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.85s NLL: 3.2024943829 Reg.: 0.0002645317 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3887932300567627\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -1.1685127019882202\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 9.787198066711426\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 27\n",
      "\t\tRight Flank Length: 27\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -4.007333278656006\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.11s NLL: 3.2355573177 Reg.: 0.0001830768 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.02s NLL: 3.2355573177 Reg.: 0.0001830768 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -4.007224082946777\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -4.0585174560546875\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 12.588062286376953\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 1.39s NLL: 3.2020771503 Reg.: 0.0002644056 Distance: 3.9829068184 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.47s NLL: 3.2020771503 Reg.: 0.0002644056 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.389876365661621\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -1.2437539100646973\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 9.771204948425293\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 28\n",
      "\t\tRight Flank Length: 28\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -4.043051242828369\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.12s NLL: 3.2355573177 Reg.: 0.0001833643 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.03s NLL: 3.2355573177 Reg.: 0.0001833643 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -4.042941093444824\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -4.094234466552734\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 12.53676986694336\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 1.60s NLL: 3.2022080421 Reg.: 0.0002641389 Distance: 3.9414234161 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.62s NLL: 3.2022080421 Reg.: 0.0002641389 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3895740509033203\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -1.3089616298675537\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 9.749091148376465\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "### Regularization:\n",
      "\t L1 Lambda: 0\n",
      "\t L2 Lambda: 1e-06\n",
      "\t Pseudocount: 0\n",
      "\t Exponential Bound: 40\n",
      "\t Excluded Reg.: frozenset()\n",
      "\t Eq. Contribution: False\n",
      "\t Weights: [1.0]\n",
      "\n",
      "### Aggregates:\n",
      "\tAggregate: 0thASBAggregate\n",
      "\n",
      "### Binding Components:\n",
      "\t Mode 0: (NSNonSpecific,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→0thContribution\n",
      "\t Mode 1: (CTCFPSAM,)\n",
      "\t\tFound In\n",
      "\t\t\t0thASBAggregate→1stContribution\n",
      "\n",
      "### Tables:\n",
      "\tTable: 0\n",
      "\t\tLeft Flank Length: 29\n",
      "\t\tRight Flank Length: 29\n",
      "\n",
      "### Training Mode 0: NSNonSpecific\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→0thContribution → 0thASBAggregate→NSNonSpecificMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→0thContribution)\n",
      "\t1.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -1.0\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -4.077537536621094\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 0.11s NLL: 3.2355573177 Reg.: 0.0001836443 Distance: 1.3126220703 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.03s NLL: 3.2355573177 Reg.: 0.0001836443 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -4.077426433563232\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=False -inf\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n",
      "\n",
      "### Training Mode 1: CTCFPSAM\n",
      "\tASBLoss → 0thASBAggregate → 0thASBAggregate→1stContribution → 0thASBAggregate→CTCFPSAMMode\n",
      "\t0.\tASBLoss.freeze()\n",
      "\t\t0thASBAggregate.activity_heuristic(contribution=0thASBAggregate→1stContribution)\n",
      "\t1.\tCTCFPSAM.unfreeze(parameter=monomer)\n",
      "\t2.\tASBLoss.unfreeze(parameter=all)\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3126220703125\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -4.128719806671143\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 12.487979888916016\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884])\n",
      "\t\t\tLoss decreased\n",
      "\t\t\tEpoch 0 took 2.76s NLL: 3.2021262646 Reg.: 0.0002638432 Distance: 3.9054012299 Patience: 10\n",
      "\t\t\tEpoch 1 took 0.72s NLL: 3.2021262646 Reg.: 0.0002638432 Distance: 0.0000000000 Patience: 9\n",
      "\t\t\t\taggregates.0.log_target_concentration grad=False 0.0\n",
      "\t\t\t\taggregates.0.log_rho grad=True -2.3885693550109863\n",
      "\t\t\t\taggregates.0.contributions.0.log_activity grad=True -1.368741750717163\n",
      "\t\t\t\taggregates.0.contributions.0.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.log_activity grad=True 9.725931167602539\n",
      "\t\t\t\taggregates.0.contributions.1.binding.log_hill grad=False 0.0\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.log_posbias grad=False\n",
      "\t\t\t\t\ttensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "\t\t\t                 [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])\n",
      "\t\t\t\taggregates.0.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False\n",
      "\t\t\t\t\ttensor([-0.8893, -0.1770,  0.0000, -0.1907,  0.0000, -0.2736, -0.3222, -0.5090,\n",
      "\t\t\t                -1.0943, -1.2095,  0.0000, -1.4176, -0.5818,  0.0000, -1.5835, -0.9661,\n",
      "\t\t\t                -2.2910, -2.1749,  0.0000, -1.8172, -1.9136,  0.0000, -2.0211, -2.5375,\n",
      "\t\t\t                -2.8599,  0.0000, -3.1848, -2.7933, -0.4245,  0.0000, -1.7129, -1.9087,\n",
      "\t\t\t                -2.1157,  0.0000, -3.1694, -0.7548, -3.0110,  0.0000, -3.2083, -2.8090,\n",
      "\t\t\t                -2.0195, -1.8831, -2.2465,  0.0000,  0.0000, -1.5585, -0.6211, -1.0109,\n",
      "\t\t\t                -1.9003, -0.8337,  0.0000, -1.1798, -0.7382,  0.0000, -1.2307, -0.0124,\n",
      "\t\t\t                -3.0011, -1.9570,  0.0000, -1.7343, -1.3595, -1.9099,  0.0000, -1.1142,\n",
      "\t\t\t                -0.6645, -0.2762, -1.0553,  0.0000, -0.4349,  0.0000, -0.3053, -0.6884]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "losses = []\n",
    "regs = []\n",
    "rhos = []\n",
    "for length in range(math.floor(psam.kernel_size / 2), 30):\n",
    "    psam._layers.clear()\n",
    "    psam.train_betas = False\n",
    "\n",
    "    asb_table.set_flank_length(length, length)\n",
    "\n",
    "    aggregate = asb.ASBAggregate(\n",
    "        (\n",
    "            pyprobound.Contribution(bmd)\n",
    "            for bmd in [\n",
    "                pyprobound.Mode(\n",
    "                    [\n",
    "                        pyprobound.layers.Conv0d.from_nonspecific(\n",
    "                            nonspecific, asb_table\n",
    "                        )\n",
    "                    ]\n",
    "                ),\n",
    "                pyprobound.Mode(\n",
    "                    [pyprobound.layers.Conv1d.from_psam(psam, asb_table)]\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    model = asb.ASBLoss([aggregate])\n",
    "    optimizer = pyprobound.Optimizer(\n",
    "        model,\n",
    "        [asb_table],\n",
    "        greedy_threshold=0,\n",
    "        device=\"cpu\",\n",
    "        checkpoint=\"test_lengths.pt\",\n",
    "    )\n",
    "    optimizer.train_sequential()\n",
    "    with torch.inference_mode():\n",
    "        loss, reg = model([asb_table])\n",
    "        rho = torch.exp(aggregate.log_rho)\n",
    "    lengths.append(length)\n",
    "    losses.append(loss)\n",
    "    regs.append(reg)\n",
    "    rhos.append(rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 430,
       "width": 579
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(lengths, losses)\n",
    "plt.ylim((0.999 * min(losses), 1.001 * max(losses)))\n",
    "plt.ylabel(\"Negative log-likelihood\")\n",
    "plt.xlabel(\"Length of sequence flanking SNV on each side\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
