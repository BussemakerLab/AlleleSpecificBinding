### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Pseudocount: 0
	 Exponential Bound: 40
	 Excluded Reg.: frozenset()
	 Eq. Contribution: False
	 Weights: [1.0]

### Experiments:
	Experiment: 0thExperiment
		Rounds: [0thExperiment→0thInitialRound, 0thExperiment→1stBoundUnsaturatedRound]

### Binding Components:
	 Mode 0: (NSNonSpecific,)
		0thExperiment→1stBoundUnsaturatedRound→Aggregate→0thContribution
	 Mode 1: (CTCFPSAM,)
		0thExperiment→1stBoundUnsaturatedRound→Aggregate→1stContribution

### Tables:
	Table: 0
		Maximum Variable Length: 200
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NSNonSpecific
	MultiExperimentLoss → 0thExperiment → 0thExperiment→1stBoundUnsaturatedRound → 0thExperiment→1stBoundUnsaturatedRound→Aggregate → 0thExperiment→1stBoundUnsaturatedRound→Aggregate→0thContribution → 0thExperiment→NSNonSpecificMode
	0.	MultiExperimentLoss.freeze()
		0thExperiment→1stBoundUnsaturatedRound→Aggregate.activity_heuristic(contribution=0thExperiment→1stBoundUnsaturatedRound→Aggregate→0thContribution)
		0thExperiment→1stBoundUnsaturatedRound.unfreeze(parameter=depth)
				experiments.0.rounds.0.log_depth grad=False 0.0
				experiments.0.rounds.1.log_depth grad=True 0.0
				experiments.0.rounds.1.aggregate.log_target_concentration grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.log_activity grad=False -5.2983174324035645
				experiments.0.rounds.1.aggregate.contributions.0.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.log_activity grad=False -inf
				experiments.0.rounds.1.aggregate.contributions.1.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([ 0.0844,  0.0397,  0.0082, -0.0928,  0.1132, -0.0382, -0.0916,  0.0339,
			                 0.0635,  0.0628, -0.0951,  0.0689, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393,  0.1179, -0.0393, -0.0393,  0.0680,  0.0680, -0.0680, -0.0680,
			                -0.0680,  0.0680, -0.0680,  0.0680, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393, -0.0393,  0.1179,  0.1179, -0.0393, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393, -0.0393,  0.1179,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393,  0.1179, -0.0393,
			                 0.0471,  0.0127,  0.0462, -0.1002,  0.0566, -0.1176,  0.0109, -0.0145])
			Loss decreased
			Epoch 0 took 0.84s NLL: 1.6467543840 Reg.: 0.0000285968 Distance: 0.4347406626 Patience: 10
			Epoch 1 took 0.26s NLL: 1.6467543840 Reg.: 0.0000285968 Distance: 0.0000000000 Patience: 9
				experiments.0.rounds.0.log_depth grad=False 0.0
				experiments.0.rounds.1.log_depth grad=True 0.43474066257476807
				experiments.0.rounds.1.aggregate.log_target_concentration grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.log_activity grad=False -5.2983174324035645
				experiments.0.rounds.1.aggregate.contributions.0.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.log_activity grad=False -inf
				experiments.0.rounds.1.aggregate.contributions.1.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([ 0.0844,  0.0397,  0.0082, -0.0928,  0.1132, -0.0382, -0.0916,  0.0339,
			                 0.0635,  0.0628, -0.0951,  0.0689, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393,  0.1179, -0.0393, -0.0393,  0.0680,  0.0680, -0.0680, -0.0680,
			                -0.0680,  0.0680, -0.0680,  0.0680, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393, -0.0393,  0.1179,  0.1179, -0.0393, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393, -0.0393,  0.1179,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393,  0.1179, -0.0393,
			                 0.0471,  0.0127,  0.0462, -0.1002,  0.0566, -0.1176,  0.0109, -0.0145]) 

	1.	MultiExperimentLoss.unfreeze(parameter=all)
				experiments.0.rounds.0.log_depth grad=False 0.0
				experiments.0.rounds.1.log_depth grad=True 0.43474066257476807
				experiments.0.rounds.1.aggregate.log_target_concentration grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.log_activity grad=True -5.2983174324035645
				experiments.0.rounds.1.aggregate.contributions.0.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.log_activity grad=False -inf
				experiments.0.rounds.1.aggregate.contributions.1.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([ 0.0844,  0.0397,  0.0082, -0.0928,  0.1132, -0.0382, -0.0916,  0.0339,
			                 0.0635,  0.0628, -0.0951,  0.0689, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393,  0.1179, -0.0393, -0.0393,  0.0680,  0.0680, -0.0680, -0.0680,
			                -0.0680,  0.0680, -0.0680,  0.0680, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393, -0.0393,  0.1179,  0.1179, -0.0393, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393, -0.0393,  0.1179,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393,  0.1179, -0.0393,
			                 0.0471,  0.0127,  0.0462, -0.1002,  0.0566, -0.1176,  0.0109, -0.0145])
			Epoch 0 took 0.27s NLL: 1.6467543840 Reg.: 0.0000285968 Distance: 0.0000000000 Patience: 9
				experiments.0.rounds.0.log_depth grad=False 0.0
				experiments.0.rounds.1.log_depth grad=True 0.43474066257476807
				experiments.0.rounds.1.aggregate.log_target_concentration grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.log_activity grad=True -5.2983174324035645
				experiments.0.rounds.1.aggregate.contributions.0.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.log_activity grad=False -inf
				experiments.0.rounds.1.aggregate.contributions.1.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([ 0.0844,  0.0397,  0.0082, -0.0928,  0.1132, -0.0382, -0.0916,  0.0339,
			                 0.0635,  0.0628, -0.0951,  0.0689, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393,  0.1179, -0.0393, -0.0393,  0.0680,  0.0680, -0.0680, -0.0680,
			                -0.0680,  0.0680, -0.0680,  0.0680, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393, -0.0393,  0.1179,  0.1179, -0.0393, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393, -0.0393,  0.1179,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393,  0.1179, -0.0393,
			                 0.0471,  0.0127,  0.0462, -0.1002,  0.0566, -0.1176,  0.0109, -0.0145]) 


### Training Mode 1: CTCFPSAM
	MultiExperimentLoss → 0thExperiment → 0thExperiment→1stBoundUnsaturatedRound → 0thExperiment→1stBoundUnsaturatedRound→Aggregate → 0thExperiment→1stBoundUnsaturatedRound→Aggregate→1stContribution → 0thExperiment→CTCFPSAMMode
	0.	MultiExperimentLoss.freeze()
		0thExperiment→1stBoundUnsaturatedRound→Aggregate.activity_heuristic(contribution=0thExperiment→1stBoundUnsaturatedRound→Aggregate→1stContribution)
		0thExperiment→1stBoundUnsaturatedRound.unfreeze(parameter=depth)
				experiments.0.rounds.0.log_depth grad=False 0.0
				experiments.0.rounds.1.log_depth grad=True 0.43474066257476807
				experiments.0.rounds.1.aggregate.log_target_concentration grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.log_activity grad=False -5.403677940368652
				experiments.0.rounds.1.aggregate.contributions.0.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.log_activity grad=False -8.229701042175293
				experiments.0.rounds.1.aggregate.contributions.1.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([ 0.0844,  0.0397,  0.0082, -0.0928,  0.1132, -0.0382, -0.0916,  0.0339,
			                 0.0635,  0.0628, -0.0951,  0.0689, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393,  0.1179, -0.0393, -0.0393,  0.0680,  0.0680, -0.0680, -0.0680,
			                -0.0680,  0.0680, -0.0680,  0.0680, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393, -0.0393,  0.1179,  0.1179, -0.0393, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393, -0.0393,  0.1179,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393,  0.1179, -0.0393,
			                 0.0471,  0.0127,  0.0462, -0.1002,  0.0566, -0.1176,  0.0109, -0.0145])
			Loss decreased
			Epoch 0 took 11.70s NLL: 1.6466883421 Reg.: 0.0000974499 Distance: 0.0027426779 Patience: 10
			Epoch 1 took 5.83s NLL: 1.6466883421 Reg.: 0.0000974499 Distance: 0.0000000000 Patience: 9
				experiments.0.rounds.0.log_depth grad=False 0.0
				experiments.0.rounds.1.log_depth grad=True 0.43199798464775085
				experiments.0.rounds.1.aggregate.log_target_concentration grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.log_activity grad=False -5.403677940368652
				experiments.0.rounds.1.aggregate.contributions.0.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.log_activity grad=False -8.229701042175293
				experiments.0.rounds.1.aggregate.contributions.1.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([ 0.0844,  0.0397,  0.0082, -0.0928,  0.1132, -0.0382, -0.0916,  0.0339,
			                 0.0635,  0.0628, -0.0951,  0.0689, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393,  0.1179, -0.0393, -0.0393,  0.0680,  0.0680, -0.0680, -0.0680,
			                -0.0680,  0.0680, -0.0680,  0.0680, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393, -0.0393,  0.1179,  0.1179, -0.0393, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393, -0.0393,  0.1179,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393,  0.1179, -0.0393,
			                 0.0471,  0.0127,  0.0462, -0.1002,  0.0566, -0.1176,  0.0109, -0.0145]) 

	1.	CTCFPSAM.unfreeze(parameter=monomer)
				experiments.0.rounds.0.log_depth grad=False 0.0
				experiments.0.rounds.1.log_depth grad=True 0.43199798464775085
				experiments.0.rounds.1.aggregate.log_target_concentration grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.log_activity grad=False -5.403677940368652
				experiments.0.rounds.1.aggregate.contributions.0.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.log_activity grad=False -8.229701042175293
				experiments.0.rounds.1.aggregate.contributions.1.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ 0.0844,  0.0397,  0.0082, -0.0928,  0.1132, -0.0382, -0.0916,  0.0339,
			                 0.0635,  0.0628, -0.0951,  0.0689, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393,  0.1179, -0.0393, -0.0393,  0.0680,  0.0680, -0.0680, -0.0680,
			                -0.0680,  0.0680, -0.0680,  0.0680, -0.0393,  0.1179, -0.0393, -0.0393,
			                -0.0393, -0.0393, -0.0393,  0.1179,  0.1179, -0.0393, -0.0393, -0.0393,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393, -0.0393,  0.1179,
			                -0.0393, -0.0393,  0.1179, -0.0393, -0.0393, -0.0393,  0.1179, -0.0393,
			                 0.0471,  0.0127,  0.0462, -0.1002,  0.0566, -0.1176,  0.0109, -0.0145])
			Loss decreased
			Epoch 0 took 404.36s NLL: 1.6420032978 Reg.: 0.0001227567 Distance: 4.7532873154 Patience: 10
			Loss decreased
			Epoch 1 took 29.50s NLL: 1.6420021057 Reg.: 0.0001229693 Distance: 0.1197789237 Patience: 10
			Loss decreased
			Epoch 2 took 47.59s NLL: 1.6420018673 Reg.: 0.0001229898 Distance: 0.0171248652 Patience: 10
			Epoch 3 took 39.45s NLL: 1.6420018673 Reg.: 0.0001229898 Distance: 0.0000000000 Patience: 9
				experiments.0.rounds.0.log_depth grad=False 0.0
				experiments.0.rounds.1.log_depth grad=True 0.33881717920303345
				experiments.0.rounds.1.aggregate.log_target_concentration grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.log_activity grad=False -5.403677940368652
				experiments.0.rounds.1.aggregate.contributions.0.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.log_activity grad=False -8.229701042175293
				experiments.0.rounds.1.aggregate.contributions.1.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.5477, -0.0219, -0.0030,  0.1564,  0.2292, -0.4882,  0.2556, -0.4314,
			                -0.6047,  0.0310,  0.4601, -0.2518, -1.0319,  0.9587, -0.7182,  0.3422,
			                -0.6537, -0.6924,  1.0236, -0.1267, -0.0783,  0.8923, -0.4710, -0.7922,
			                -0.0633,  0.8663,  0.0162, -1.2685,  0.2355,  0.5709,  0.0501, -1.3058,
			                -0.5527,  0.7132, -1.0921,  0.4824, -0.4029,  1.0413, -0.1564, -0.9313,
			                -0.5460, -0.0594, -0.6952,  0.8514,  0.2380, -0.3527,  0.1496, -0.4841,
			                -1.1133,  0.2803,  0.8107, -0.4269, -0.5630,  0.3336, -0.6527,  0.4329,
			                -1.0364, -0.0998,  1.2021, -0.5151, -0.3279, -0.4864,  0.6644, -0.2993,
			                -0.3054,  0.3564, -0.4315, -0.0639, -0.2314,  0.0913, -0.1002, -0.2630]) 

	2.	0thExperiment→CTCFPSAMMode→Layer0:CTCFPSAM←Conv1d.unfreeze(parameter=posbias)
				experiments.0.rounds.0.log_depth grad=False 0.0
				experiments.0.rounds.1.log_depth grad=True 0.33881717920303345
				experiments.0.rounds.1.aggregate.log_target_concentration grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.log_activity grad=False -5.403677940368652
				experiments.0.rounds.1.aggregate.contributions.0.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.log_activity grad=False -8.229701042175293
				experiments.0.rounds.1.aggregate.contributions.1.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.log_posbias grad=True
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.5477, -0.0219, -0.0030,  0.1564,  0.2292, -0.4882,  0.2556, -0.4314,
			                -0.6047,  0.0310,  0.4601, -0.2518, -1.0319,  0.9587, -0.7182,  0.3422,
			                -0.6537, -0.6924,  1.0236, -0.1267, -0.0783,  0.8923, -0.4710, -0.7922,
			                -0.0633,  0.8663,  0.0162, -1.2685,  0.2355,  0.5709,  0.0501, -1.3058,
			                -0.5527,  0.7132, -1.0921,  0.4824, -0.4029,  1.0413, -0.1564, -0.9313,
			                -0.5460, -0.0594, -0.6952,  0.8514,  0.2380, -0.3527,  0.1496, -0.4841,
			                -1.1133,  0.2803,  0.8107, -0.4269, -0.5630,  0.3336, -0.6527,  0.4329,
			                -1.0364, -0.0998,  1.2021, -0.5151, -0.3279, -0.4864,  0.6644, -0.2993,
			                -0.3054,  0.3564, -0.4315, -0.0639, -0.2314,  0.0913, -0.1002, -0.2630])
			Epoch 0 took 35.54s NLL: 1.6420018673 Reg.: 0.0001229898 Distance: 0.0000000000 Patience: 9
				experiments.0.rounds.0.log_depth grad=False 0.0
				experiments.0.rounds.1.log_depth grad=True 0.33881717920303345
				experiments.0.rounds.1.aggregate.log_target_concentration grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.log_activity grad=False -5.403677940368652
				experiments.0.rounds.1.aggregate.contributions.0.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.log_activity grad=False -8.229701042175293
				experiments.0.rounds.1.aggregate.contributions.1.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.log_posbias grad=True
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.5477, -0.0219, -0.0030,  0.1564,  0.2292, -0.4882,  0.2556, -0.4314,
			                -0.6047,  0.0310,  0.4601, -0.2518, -1.0319,  0.9587, -0.7182,  0.3422,
			                -0.6537, -0.6924,  1.0236, -0.1267, -0.0783,  0.8923, -0.4710, -0.7922,
			                -0.0633,  0.8663,  0.0162, -1.2685,  0.2355,  0.5709,  0.0501, -1.3058,
			                -0.5527,  0.7132, -1.0921,  0.4824, -0.4029,  1.0413, -0.1564, -0.9313,
			                -0.5460, -0.0594, -0.6952,  0.8514,  0.2380, -0.3527,  0.1496, -0.4841,
			                -1.1133,  0.2803,  0.8107, -0.4269, -0.5630,  0.3336, -0.6527,  0.4329,
			                -1.0364, -0.0998,  1.2021, -0.5151, -0.3279, -0.4864,  0.6644, -0.2993,
			                -0.3054,  0.3564, -0.4315, -0.0639, -0.2314,  0.0913, -0.1002, -0.2630]) 

	3.	MultiExperimentLoss.unfreeze(parameter=all)
				experiments.0.rounds.0.log_depth grad=False 0.0
				experiments.0.rounds.1.log_depth grad=True 0.33881717920303345
				experiments.0.rounds.1.aggregate.log_target_concentration grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.log_activity grad=True -5.403677940368652
				experiments.0.rounds.1.aggregate.contributions.0.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.log_activity grad=True -8.229701042175293
				experiments.0.rounds.1.aggregate.contributions.1.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.log_posbias grad=True
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.5477, -0.0219, -0.0030,  0.1564,  0.2292, -0.4882,  0.2556, -0.4314,
			                -0.6047,  0.0310,  0.4601, -0.2518, -1.0319,  0.9587, -0.7182,  0.3422,
			                -0.6537, -0.6924,  1.0236, -0.1267, -0.0783,  0.8923, -0.4710, -0.7922,
			                -0.0633,  0.8663,  0.0162, -1.2685,  0.2355,  0.5709,  0.0501, -1.3058,
			                -0.5527,  0.7132, -1.0921,  0.4824, -0.4029,  1.0413, -0.1564, -0.9313,
			                -0.5460, -0.0594, -0.6952,  0.8514,  0.2380, -0.3527,  0.1496, -0.4841,
			                -1.1133,  0.2803,  0.8107, -0.4269, -0.5630,  0.3336, -0.6527,  0.4329,
			                -1.0364, -0.0998,  1.2021, -0.5151, -0.3279, -0.4864,  0.6644, -0.2993,
			                -0.3054,  0.3564, -0.4315, -0.0639, -0.2314,  0.0913, -0.1002, -0.2630])
			Epoch 0 took 36.10s NLL: 1.6420018673 Reg.: 0.0001229898 Distance: 0.0000000000 Patience: 9
				experiments.0.rounds.0.log_depth grad=False 0.0
				experiments.0.rounds.1.log_depth grad=True 0.33881717920303345
				experiments.0.rounds.1.aggregate.log_target_concentration grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.log_activity grad=True -5.403677940368652
				experiments.0.rounds.1.aggregate.contributions.0.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.log_activity grad=True -8.229701042175293
				experiments.0.rounds.1.aggregate.contributions.1.binding.log_hill grad=False 0.0
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.log_posbias grad=True
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				experiments.0.rounds.1.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.5477, -0.0219, -0.0030,  0.1564,  0.2292, -0.4882,  0.2556, -0.4314,
			                -0.6047,  0.0310,  0.4601, -0.2518, -1.0319,  0.9587, -0.7182,  0.3422,
			                -0.6537, -0.6924,  1.0236, -0.1267, -0.0783,  0.8923, -0.4710, -0.7922,
			                -0.0633,  0.8663,  0.0162, -1.2685,  0.2355,  0.5709,  0.0501, -1.3058,
			                -0.5527,  0.7132, -1.0921,  0.4824, -0.4029,  1.0413, -0.1564, -0.9313,
			                -0.5460, -0.0594, -0.6952,  0.8514,  0.2380, -0.3527,  0.1496, -0.4841,
			                -1.1133,  0.2803,  0.8107, -0.4269, -0.5630,  0.3336, -0.6527,  0.4329,
			                -1.0364, -0.0998,  1.2021, -0.5151, -0.3279, -0.4864,  0.6644, -0.2993,
			                -0.3054,  0.3564, -0.4315, -0.0639, -0.2314,  0.0913, -0.1002, -0.2630]) 

